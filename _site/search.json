[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inar’s blog",
    "section": "",
    "text": "Speed of Llama\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nInar Timiryasov\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Inar Timiryasov",
    "section": "",
    "text": "I am a postdoc at the University of Copenhagen working on the intersection of AI and particle physics.\nMy CV"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Inar Timiryasov",
    "section": "Work Experience",
    "text": "Work Experience\n\nPostdoctoral Researcher (2021–present)\nNiels Bohr Institute, University of Copenhagen\nDeveloping AI models for applications in particle physics, including methods to reinterpret LHC data analysis pipelines.\nI also trained Baby Llamas!\nVisiting Researcher (2016–present)\nCERN\nMember of the SHiP collaboration searching for new feebly interacting particles.\nPostdoctoral Researcher (2016–2021)\nÉcole Polytechnique Fédérale de Lausanne\nWorked on transforming theoretical problems into computable models, taught courses, and supervised Master’s students."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/speed-of-llama/index.html",
    "href": "posts/speed-of-llama/index.html",
    "title": "Speed of Llama",
    "section": "",
    "text": "This summer, I participated in the BabyLM Challenge, which aimed to improve the sample efficiency of language models by training them on a small (10M or 100M words), developmentally-plausible dataset.\nEventually, we trained an ensemble consisting of a GPT-2 (705M parameters) and LLaMA (360M parameters) models and then distilled it into a small, 58M-parameter LLaMA model. This model exceeds in performance both of its teachers as well as a similar model trained without distillation. The models were benchmarked using BLiMP, (Super)GLUE, and MSGS tasks. Our BabyLlama scored in the top 5% and is the best decoder model in the competition! Frankly, encoder models, such as Roberta and BERT, were much stronger in BLiMP. Also, the first-place model implemented a very interesting modification of the usual BERT architecture, using a weighted sum of activations of all previous layers as a layer input. Check the paper for details. I am currently exploring a similar approach for a decoder model.\nOur code for training HF Transformers models on the BabyLM dataset as well as for distillation pretraining is available here."
  },
  {
    "objectID": "posts/speed-of-llama/index.html#babylm-challenge",
    "href": "posts/speed-of-llama/index.html#babylm-challenge",
    "title": "Speed of Llama",
    "section": "",
    "text": "This summer, I participated in the BabyLM Challenge, which aimed to improve the sample efficiency of language models by training them on a small (10M or 100M words), developmentally-plausible dataset.\nEventually, we trained an ensemble consisting of a GPT-2 (705M parameters) and LLaMA (360M parameters) models and then distilled it into a small, 58M-parameter LLaMA model. This model exceeds in performance both of its teachers as well as a similar model trained without distillation. The models were benchmarked using BLiMP, (Super)GLUE, and MSGS tasks. Our BabyLlama scored in the top 5% and is the best decoder model in the competition! Frankly, encoder models, such as Roberta and BERT, were much stronger in BLiMP. Also, the first-place model implemented a very interesting modification of the usual BERT architecture, using a weighted sum of activations of all previous layers as a layer input. Check the paper for details. I am currently exploring a similar approach for a decoder model.\nOur code for training HF Transformers models on the BabyLM dataset as well as for distillation pretraining is available here."
  },
  {
    "objectID": "posts/speed-of-llama/index.html#llamas-are-fast",
    "href": "posts/speed-of-llama/index.html#llamas-are-fast",
    "title": "Speed of Llama",
    "section": "Llamas are Fast",
    "text": "Llamas are Fast\nDuring my experiments, I trained many different models from scratch. I found that Llama trains significantly faster than GPT-2. It reaches the minimum eval loss in nearly half the number of epochs needed for GPT-2.\nThis made me curious: what is the reason? There are two main differences between the models: GPT uses trainable positional embeddings, while Llama employs Rotary Positional Embedding (RoPE). Additionally, Llama utilizes SwiGLU instead of simple MLP layers.\nTo try to isolate these two effects, I also trained GPT-J, which uses RoPE (although I used the default settings and didn’t attempt to make the RoPE implementations match precisely) but not SwiGLU. To make the comparison with GPT-2 more accurate, I enabled weight tying in both Llama and GPT-J (this feature is disabled by default). I performed a grid search for the optimal learning rate (which happened to be the same for all three models) using the 10M BabyLM dataset (strict-small task). Then, I trained all the models using the 100M dataset (strict task; see the configs *-strict.yaml). The result is shown below.\n\n\n\neval-loss\n\n\nLlama achieves a lower loss than GPT-J and does so more quickly than GPT-2. It seems that SwiGLU—a gated unit that is quadratic in its inputs—is the key to the performance gain."
  },
  {
    "objectID": "posts/speed-of-llama/index.html#swiglu-capacity",
    "href": "posts/speed-of-llama/index.html#swiglu-capacity",
    "title": "Speed of Llama",
    "section": "SwiGLU capacity",
    "text": "SwiGLU capacity\nNext, I plan to study the capacity of SwiGLU layers. By capacity, I mean—following David MacKay’s book—the amount of information a network can store. That is, let’s take some random vectors and random binary labels. Then we overfit a network to this data. The capacity is the number of different vectors the model can memorize. This is closely related to Rademacher complexity. What I’ve found so far is that a SwiGLU network has a larger complexity than a simple MLP."
  }
]
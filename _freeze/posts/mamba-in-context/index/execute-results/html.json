{
  "hash": "17227b5e8b4b7a888166f2751e00edf3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Mamba and in-context learing: initial look\"\n# description: \"Post description\"\nauthor: \n  - \"Inar Timiryasov\"\n  - \"Jean-Loup Tastet\"\ndate: \"2024-02-10\"\nbibliography: SSSM_refs.bib\ndraft: true\nexecute:\n  echo: false\n# reference-location: margin\n# categories:\n# - LLM\n# format:\n#   pdf:\n#     toc: true\n#     number-sections: true\n#     colorlinks: true\n---\n\n<!-- todo: \n- redo plots\n- html with predictions\n- cite the recent paper with vocab size = 40\n-->\n\n# DRAFT 2024-02-12\n\n## **Introduction**\n\nMamba [@gu2023mamba] is an architecture based on a Selective Structured State-Space model.\nRecently it has taken the community by storm --- and for a good reason. It features linear complexity in the sequence length and outperforms transformers of similar size in the language modelling task. It also benefits from a hardware-aware implementation. It has already been applied to tasks such as medical image segmentation [@ma2024u]. \nWe are now applying Mamba to study ultra-high energy particles in a gigantic neutrino telescope in the South pole.^[This telescope is an instrumented cubic kilometer of ice, called IceCube. It has more than 5000 photodetectors that collect the data.\nTransformers are in general well suited for this task. But their quadratic dependence on the input length doesn't allow to study the brightest events that have hundreds of thousands of pulses.]\n\nInterestingly, the model design has been in part motivated by the mechanistic interpretability [@elhage2021mathematical] studies and specifically the idea of in-context learning and induction heads [@olsson2022context]. \n\nSo here we wanted take a look at the in-context learning in Mamba. Meanwhile, there are already two papers with seemingly contradicting results. @akyurek2024context studied synthetic languages with small vocabulary size and found that transformers are superior to other models. On the other hand, @grazzi2024mamba find thath Mamba is capable of in-context learning. We wanted to take a look at this ourselves.\n\n\n\n## **In-Context Learning**\n\nBy in-context learning we mean the ability of the model to learn during the inference time using the information from the context. This most clearly manifests itself as the decrease of the per-token loss as a function of the token position in the sequence, see @fig-in-context.\n\n<!-- placeholder: create a better plot\n![Loss per token as a function of token position, averaged over 200 sequences. Children stories.](fig/incontext-loss.png){fig-align=\"center\" width=60% #fig-in-context} -->\n\n::: {#cell-fig-in-context .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.](index_files/figure-html/fig-in-context-output-1.png){#fig-in-context width=576 height=429}\n:::\n:::\n\n\nInduction heads are believed to be central to in-context learning. So what are they?\nInduction heads are circuits that allow the model to predict [B] after [A] if the pair [A][B] has already appeared in the context.^[Think of [A] being \"Harry\" and [B] being \"Potter\". If \"Harry Potter\" was present in the context already, the model will predict \"Potter\" after \"Harry\" with high probability.]\nInduction heads are already present in two layer attention-only transformers (but not in single layer ones). \n\n<!-- add predictions html-->\n\nMamba and the predecessor model H3 [@fu2022hungry] have been designed with the idea of induction heads in mind.\n\n\n\n## **Mamba: phenomenological study**\nHow can we check the in-context learning ability of Mamba?\nOne simple test is to feed the model random garbage, repeated twice. Quite amusingly, transformers are able to learn the pattern and predict the second half of the sequence with high accuracy, like in @fig-random-qwen. We used the smallest [Qwen-1.5](https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524) model mainly since it has long context window which will be used below. Even GPT-2 124M is perfectly capable of solving this task as far as the sequence fits into the context length.\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/qwen-random-tokens.png){fig-align=\"center\" width=60% #fig-random-qwen} -->\n\n::: {#cell-fig-random-qwen .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-qwen-output-1.png){#fig-random-qwen width=585 height=429}\n:::\n:::\n\n\nSo what about Mamba? We used the [original](https://huggingface.co/state-spaces/mamba-790m) model which has been trained with 2k context length. Recently, [LongMamba](https://github.com/jzhang38/LongMamba) with 16k context has appeared, so we also used this [model](https://huggingface.co/PY007/LongMamba_16384_bs128_step400/tree/main). Here is the result:\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens.png){fig-align=\"center\" width=60% #fig-random-mamba} -->\n\n::: {#cell-fig-random-mamba .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-mamba-output-1.png){#fig-random-mamba width=585 height=429}\n:::\n:::\n\n\nOk, what about even longer contexts?\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens-long.jpeg){fig-align=\"center\" width=60% #fig-random-mamba-long} -->\n\n::: {#cell-fig-random-mamba-long .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-mamba-long-output-1.png){#fig-random-mamba-long width=585 height=429}\n:::\n:::\n\n\nOops! So, we can see that a small transformer deals with this easily, but Mamba struggles. Anyway, this is quite relatable -- transformers are clearly superhuman in memorising random crap.\n\nNotice that this is a very artificial test, though. The data is completely out of distribution.^[Garbage in --- garbage out. It is actually really impressive that transformers deal with this so easily.] One can speculate that perhaps mamba learns N-gram statistics too well and the induction heads cannot change the predictions. \n\nSo what about normal texts? We used Children Stories dataset which was a part of the [BabyLM Challenge](https://babylm.github.io). All stories are in a single file, which we tokenized and split into sequences of 8000 tokens. So the beggining of the sequences is not sequences are not aligned with the text in a minigful way. This is probably the reason why the plots are noisy.\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-losses layout-ncol=2}\n\n![Qwen](fig/incontext-loss.png){width=90%}\n\n![Mamba](fig/incontext-loss-mamba.png){width=90%}\n\nLoss per token as a function of token position, averaged over 200 sequences. Children stories.\n::: -->\n\n::: {#cell-fig-losses-stories .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.](index_files/figure-html/fig-losses-stories-output-1.png){#fig-losses-stories width=756 height=374}\n:::\n:::\n\n\nAs one can see from @fig-losses-stories, the original Mamba strugles with longer sequnces. This is not so surprising since the model was trained with the 2k context length. LongMamba [post](https://github.com/jzhang38/LongMamba) has a nice intuition why this may happen. LonMamba performs on-par with transformer. So we do see the decrease of the per-token loss which signifies in-context learning. Can we quantify this?\n\n## In-Context Learning During Training\n\nTODO:\n\n* discuss the observations by @olsson2022context\n\n* introduce the score\n\n* Describe the models and the dataset.\n\n* explain why separating ssm (see the next subsection)\n\n### **In-Context Learning Score**\n\nIntroduce the score...\n\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-scores layout-ncol=2}\n\n![](fig/score.png){width=90%}\n\n![](fig/loss.png){width=90%}\n\nScore and eval loss.\n::: -->\n\n::: {#cell-fig-scores .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![In-context learning score and eval loss for Mamba and attention-only transformers.](index_files/figure-html/fig-scores-output-1.png){#fig-scores width=765 height=508}\n:::\n:::\n\n\n## ** SSSM-only models**\n\n\nMamba is more intricate than a transformer since it involves a selective state space model and gating, see [@gu2023mamba].\nIt is known, however, that attention-only transformers already exhibit interesting behavior [@elhage2021mathematical]. Can we, analogously, consider a model based on the SSM only, without convolutions and gating?\n\nWe have trained several models on PG-19 dataset [@raecompressive2019]. Those included: single-layer and two-layer attention-only transformers with rotary position encoding (RoPE), Mamba, and SSM-only models. The training of Mamba and attention-only transformers is shown in figure @fig-scores, while SSM-only models and attention-only transformers are compared in figure @fig-scores-ssm.\n\n\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-scores-ssm layout-ncol=2}\n\n![](fig/score-ssm.png){width=90%}\n\n![](fig/loss-ssm.png){width=90%}\n\nIn-context learning score and eval loss for SSMs and attention-only transformers.\n::: -->\n\n::: {#cell-fig-scores-ssm .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![In-context learning score and eval loss for SSMs and attention-only transformers.](index_files/figure-html/fig-scores-ssm-output-1.png){#fig-scores-ssm width=765 height=508}\n:::\n:::\n\n\nSeveral observations are in order. First,  we can see that an SSM-only model is already pretty solid at language modelling. Secondly, both the single-layer attention-only transformer and the SSM struggle with in-context learning. This is expected since we know that induction heads can only form in two-layer models [@elhage2021mathematical]. At the same time, the SSM has a lower loss.^[We use the same tokenizer and the same model size, with only the learning rate tuned for each model separately.]\nAt two layers things become interesting. The transformer experiences a \"phase transition\" and the score quickly drops.^[Our plots agree very well with [@olsson2022context] except that the phase change happens earlier in training. We use a rather small vocab size of 16384 and perhaps much smaller batch size of 24 sequence of 2048 tokens.]\nBut for the 2-layer SSM the drop is far less significant. In fact, we have checked SSMs up to 16 layers and they cannot match the score of the two-layer attention-only transoformer. This is in contrast to the loss, which is lower for SSMs. One can speculate that selective SSMs are better at approximating N-gram statistics, but worse at in-context learning. Can we try to understand why is this the case? \n\n\n## Discussion\n\nWe have seen that Mamba is capable of in-context learning, but not as good as transformers. One **very handwavy** explanation is that Mamba has to compress all the information from the previous tokens into a single state. What is the size of this state? There is a hidden state vector (with default size `d_state=16`) per every coordinate of the input vector. The input vector has size `2 * d_model`^[This is because the embedding vector of every token is expanded by a factor of 2 before passing to the SSM, so the input vector is twice as large as the embedding vector.] Bare in mind also that those hidden state vectors do not talk to each other explicitly. So the total size of the state is `2 * d_state * d_model`. \n\nIn the attention mechanism, on the other hand, we have to pass the KV cache from all previous tokens. This is `2 * d_model * context_length`. So the amount of information^[We are sloppy here. Transformers and SSM compress information in different ways, so the diensioanlity of the cache does not necessary transforms into information in the strict mathematical sense.] that attetnion passes to the next token surpasses that of SSM once the context is longer than `d_state`. Again, let us stress that this is a specualtion so far.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
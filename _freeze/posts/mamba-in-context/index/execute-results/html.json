{
  "hash": "a3a57a25086842336d86ed43dee55fc5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Mamba and in-context learing: initial look\"\n# description: \"Post description\"\nauthor: \n  - \"Inar Timiryasov\"\n  - \"Jean-Loup Tastet\"\ndate: \"2024-02-10\"\nbibliography: SSSM_refs.bib\ndraft: true\nexecute:\n  echo: false\n# categories:\n# - LLM\n# format:\n#   pdf:\n#     toc: true\n#     number-sections: true\n#     colorlinks: true\n---\n\n<!-- todo: \n- redo plots\n- html with predictions\n- cite the recent paper with vocab size = 40\n-->\n\n## **Introduction**\n\nMamba [@gu2023mamba] is an architecture based on a Selective Structured State-Space model.\nRecently it has taken the community by storm -- and for a good reason. It features linear complexity in the sequence length and outperforms transformers of the similar size in the language modelling task. It also benefits from a hardware-aware implementation. It has already been applied to tasks such as medical image segmentation [@ma2024u]. We are excited to see if Mamba could be used to study ultra-high energy particles in a gigantic neutrino telescope in the South pole.^[This telescope is an instrumented cubic kilometer of ice, called IceCube. It has more than 5000 photo detectors that collect the data.\nTransformers are in general well suited for this task. But their quadratic dependence on the input length doesn't allow to study the brightest events that have hundreds of thousands of pulses.]\n\nInterestingly, the model design has been in part motivated by the mechanistic interpretability [@elhage2021mathematical] studies and specifically the idea of in-context learning and induction heads [@olsson2022context]. \n\nSo here we will take a look at the in-context learning in Mamba.\n\n## **In-Context Learning**\n\nBy in-context learning we mean the ability of the model to learn during the inference time using the information from the context. This most clearly manifests itself as the decrease of the per-token loss as a function of the token position in the sequence, see Figure @fig-in-context.\n\n<!-- placeholder: create a better plot\n![Loss per token as a function of token position, averaged over 200 sequences. Children stories.](fig/incontext-loss.png){fig-align=\"center\" width=60% #fig-in-context} -->\n\n::: {#cell-fig-in-context .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of token position, averaged over 200 sequences. Children stories dataset.](index_files/figure-html/fig-in-context-output-1.png){#fig-in-context width=576 height=429}\n:::\n:::\n\n\nInduction heads are believed to be central to the in-context learning. So what are they?\nInduction heads are circuits that allow \nmodel to predict [B] after [A] if the pair [A][B] has already appeared in the context.^[Think of [A] being \"Harry\" and [B] being \"Potter\". If \"Harry Potter\" was present in the context already, the model will predict \"Potter\" after \"Harry\" with high probability.]\nInduction heads are already present in two layer attention only transformers (but not in single layer ones). \n\n<!-- add predictions html-->\n\nMamba and the predecessor model H3 [@fu2022hungry] have been designed with the idea of induction heads in mind.\n\n\n\n## **Mamba: phenomenological study**\nHow can we check the in-context learning ability of Mamba?\nOne simple test is to feed the model with garbage repeated twice. Quite amusingly, transformers are able to learn the pattern and predict the second half of the sequence with high accuracy, like in @fig-random-qwen.\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/qwen-random-tokens.png){fig-align=\"center\" width=60% #fig-random-qwen} -->\n\n::: {#cell-fig-random-qwen .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-qwen-output-1.png){#fig-random-qwen width=585 height=429}\n:::\n:::\n\n\nSo what about mamba? We used [this](https://huggingface.co/state-spaces/mamba-790m) model. Here is the result:\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens.png){fig-align=\"center\" width=60% #fig-random-mamba} -->\n\n::: {#cell-fig-random-mamba .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-mamba-output-1.png){#fig-random-mamba width=585 height=429}\n:::\n:::\n\n\nOk, what about longer contexts? The mamba should be able to handle really long sequences, right?\n\n<!-- placeholder: create a better plot-->\n<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens-long.jpeg){fig-align=\"center\" width=60% #fig-random-mamba-long} -->\n\n::: {#cell-fig-random-mamba-long .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](index_files/figure-html/fig-random-mamba-long-output-1.png){#fig-random-mamba-long width=585 height=429}\n:::\n:::\n\n\nOops! So, we can see that a small transformer deals with this easily, but Mamba struggles. Notice that this is a very artificial test though. The data is completely out of distribution.^[Garbage in -- garbage out. It is actually really impressive that transformers deal with this so easily] Maybe the model learns N-gram statistics too well and the induction heads cannot change the predictions.\n\nSo what about normal texts?\n\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-losses layout-ncol=2}\n\n![Qwen](fig/incontext-loss.png){width=90%}\n\n![Mamba](fig/incontext-loss-mamba.png){width=90%}\n\nLoss per token as a function of token position, averaged over 200 sequences. Children stories.\n::: -->\n\n::: {#cell-fig-losses .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Loss per token as a function of token position, averaged over 200 sequences. Children stories dataset.](index_files/figure-html/fig-losses-output-1.png){#fig-losses width=756 height=374}\n:::\n:::\n\n\n### **In-Context Learning Score**\n\nIntroduce the score...\n\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-scores layout-ncol=2}\n\n![](fig/score.png){width=90%}\n\n![](fig/loss.png){width=90%}\n\nScore and eval loss.\n::: -->\n\n::: {#cell-fig-scores .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![In-context learning score and eval loss for Mamba and attention-only transformers.](index_files/figure-html/fig-scores-output-1.png){#fig-scores width=758 height=374}\n:::\n:::\n\n\n## **Understanding Selective SSMs**\n\n### **Preliminaries**\nMamba is more intricate than a transformer since it involves a selective state space model and gating, see [@gu2023mamba].\nIt is known, however, that already attention-only transformers exhibit interesting behavior [@elhage2021mathematical]. Can we analogously consider a model based on the SSM, without convolutions and gating?\n\nWe have trained several models on PG-19 dataset [@raecompressive2019]. Those included: single layer and two layer transformers with RoPE position encoding, Mamba, and SSM-only models. Mamba and Transformer training is shown in figure @fig-scores, while SSMs and transformers are shown in figure @fig-scores-ssm.\n\n\n<!-- placeholder: create a better plot-->\n<!-- ::: {#fig-scores-ssm layout-ncol=2}\n\n![](fig/score-ssm.png){width=90%}\n\n![](fig/loss-ssm.png){width=90%}\n\nIn-context learning score and eval loss for SSMs and attention-only transformers.\n::: -->\n\n::: {#cell-fig-scores-ssm .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![In-context learning score and eval loss for SSMs and attention-only transformers.](index_files/figure-html/fig-scores-ssm-output-1.png){#fig-scores-ssm width=758 height=374}\n:::\n:::\n\n\nSeveral observations are in order. First,  we can see that an SSM only model is already pretty solid in language modelling. Secondly, both single layer attention-only transformer and SSM struggle with in-context learning. This is expected since we know that induction heads can only form in two-layer models [@elhage2021mathematical]. At the same time, SSM has lower loss.^[We use the same tokenizer and the same model size with only the learning rate tuned for each model separately.]\nAt two layers things become interesting. The transformer experience a \"phase transition\" and the score quickly drops.^[Our plots agree very well with [@olsson2022context] except that the phase change happens earlier in training. We use a rather small vocab size of 16384 and perhaps much smaller batch size of 24 sequence of 2048 tokens.]\nBut for the 2-layer SSM the drop is far less significant. In fact, we have checked SSMs up to 16 layers and they cannot match the score of two layer attention only transoformer. This is in contrast to the loss, which is lower for SSMs. One can speculate that Selective SSM is better at approximating N-gram statistics, but worse at in-context learning. Can we try to understand why is this the case? \n\n### **Mathematical Formulation of SSM**\nOriginally, the discovery of the induction heads became possible due to a neat mathematical formulation of transformers [@elhage2021mathematical]. Here we report our initial attempt to provide a similar formulation for the SSMs.\n\nOne difficulty that we immediately encouter is the presence of different dimensions in the model: the hidden dimension (in Mamba it is twice of the residual stream), the sequence length, and the state-space dimension. This makes it hard to distinguish between different multiplications. To make things as clear as possible, we will write all indices explicitly and use the Einstein summation convention. Furthermore, we will denote indices in such a way that it is clear what they mean.\nNamely, we will use letters $s, t, r$ for the sequence position, $i, j, k, l$ for the hidden dimension,^[Notice that Mamba expands the residual stream by a factor of two, so the hidden dimension is twice the embedding dimension.] and $\\alpha, \\beta$ for the state-space dimension. We will ignore the batch dimension since it could be trivially added.\n\n::: {.callout-tip}\n## The usual attention\nTo warm up, let's start with the usual transformer.\nThe output of one attention head can be written as\n$$\ny_{t\\,i} = A_{t\\,s}(x) \\, v_{s\\,i},\n$$\nwhere $A_{t\\,s}$ are the attention scores whcih depend on all inputs, but have only the sequence indices. Notice that we sum over the repeated indices. Writinig this equation explicitly we have\n$$\ny_{t\\,i} = \\text{softmax} \\left( x_{s\\, k} W^Q_{k\\, \\alpha}  W^K_{l\\, \\alpha} x_{t\\, l} \\right)\\; W^O_{i\\, \\beta}\\, W^V_{j\\, \\beta}\\, x_{s\\, j}.\n$$\nNotice that $ W^Q_{k\\, \\alpha}  W^K_{l\\, \\alpha}$ and $W^O_{i\\, \\beta}\\, W^V_{j\\, \\beta}$ are low-rank matrices (here $\\alpha$ goes from 1 to hidden_size/num_heads). We can rewrite the previous equation as^[See [@elhage2021mathematical], where OV and KQ compositions have been introduced.]\n$$\ny_{t\\,i} = \\text{softmax} \\left( x_{s\\, k} W^{KQ}_{k\\,l} x_{t\\, l} \\right)\\; W^{OV}_{i\\,j}\\, x_{s\\, j}.\n$${#eq-attention}\n:::\n\n\nNow, after some index gymnastic, let's move to SSMs.\nThe input to the SSM is dented $x_{s, i}$, the output is $y_{s, i}$. Notice that in  our formulation the hidden state never shows up.\nHere we list the parameters of the SSM:\n\n$$\n\\begin{aligned}\n\\Delta_{t\\, i} (x) &= \\text{softplus}(W^\\Delta_{i\\, j}\\, x_{t\\, j}),\\\\\nB_{t\\, \\alpha}(x) &= W^B_{\\alpha\\, i} \\, x_{t\\, i},\\\\\nC_{t\\, \\alpha}(x) &= W^C_{\\alpha\\, i} \\, x_{t\\, i},\\\\\n\\bar{A}_{t\\,i\\,\\alpha}(x) &= \\exp \\left(\\Delta_{t\\, i}(x)\\, A_{i\\, \\alpha}\\right),\\\\\n\\bar{B}_{t\\,i\\,\\alpha}(x) &= \\Delta_{t\\, i}(x) B_{t\\, \\alpha}(x).\\\\\n\\end{aligned}\n$${#eq-eq-ssm}\nWe sum over the dummy indices. By dummy indices we mean the indices that appear twice on *one side of the euation*.\nFor example, $B_{t\\, \\alpha}(x) = W^B_{\\alpha\\, i} \\, x_{t\\, i}$ is a shorthand for  $B_{t\\, \\alpha}(x) = \\sum_i W^B_{\\alpha\\, i} \\, x_{t\\, i}$. In matrix notations, we would write it as $B = X\\, \\left( W^B \\right)^T$. But notice that there is no sum over $i$ and $t$ in  $\\bar{A}_{t\\,i\\,\\alpha}(x) = \\exp \\left(\\Delta_{t\\, i}(x) \\cdot A_{i\\, \\alpha}\\right)$ since $i$ and $t$ appear on both the left and the right hand side of the equation. This is precisely the convention of the `torch.einsum` function.\n\n\n::: {.callout-important}\n## Selective SSM\nNow, we are ready to take a deep breath and write the output of the SSM:\n$$\ny_{t\\,i} = x_{t\\,k} \\, W_{\\alpha \\, k}^C \\; \\exp{\\left(A_{\\alpha\\, i} \\sum_{r=s+1}^{t} \\Delta_{r\\,i}(x)\\right)}\\; W_{\\alpha\\,j}^B \\,x_{s\\,j}\\; \\Delta_{s\\,i}(x)\\, x_{s\\,i}.\n$${#eq-ssm-attention}\nLet's rewrite this as\n$$\ny_{t\\,i} = q_{t\\, \\alpha} \\; g^{\\alpha\\beta}_{s\\,t\\,i}(x) \\; k_{s\\, \\beta} \\; v_{s\\,i},\n$${#eq-ssm-attention-short}\nwhere\n$$\n\\begin{aligned}\nq_{t\\, \\alpha} &=  W_{\\alpha \\, k}^C\\,x_{t\\,k} ,\\\\\nk_{s\\, \\beta} &= W_{\\alpha\\,j}^B \\,x_{s\\,j} ,\\\\\nv_{s\\,i} &= \\Delta_{s\\,i}(x)\\, x_{s\\,i},\\\\\ng^{\\alpha\\beta}_{s\\,t\\,i}(x) &= \\exp{\\left(A_{\\alpha\\, i} \\sum_{r=s+1}^{t} \\Delta_{r\\,i}(x)\\right)}\\, \\delta_{\\alpha\\beta}.\n\\end{aligned}\n$$\n:::\nThis looks very similar to attention! \n\n\nMore specifically, the linear attention without softmax. Matrices $W^B$ and $W^C$ are the analogues of the query and the key matrices in the attention mechanism. They project  from the hidden dimension to the much smaller state space dimension. Notice that this is the feature of the *selective* SSM. In the usual SSMs $B$ and $C$ do not depend on $x$ so this analogy to keys and queries is lost.\n\nThere are important differences though. First, instead of multiple attention heads with different projections, we have only one, but keys and queries are multiplied with a \"metric\" $g^{\\alpha\\beta}_{s\\,t\\,i}(x)$ which is different for every coordinate $i$ across the hidden dimension.^[Interestingly, the exponential depending on sequence positions of the source and destination tokens also appears in the case of the usual position embeddings [@tsai2019transformer], but in the usual transformers it doesn't depend on the token embeddings.]\nSo, in some sense, we have as many attention heads as there are hidden dimensions. \n\nSecondly, the value is not just a linear transformation of the token embedding as in the usual attention mechanism @eq-attention. Instead, it is gated by the matrix $\\Delta_{s\\,i}(x)$, which in turn depends on the input.\n\nThose differences might be crucial. To understand why, let's recall that there are exponentially many almost orthogonal vectors in high dimensions, see e.g.~[@Tao2013CheapKL]. So if we perform a linear transformation of the input, like in @eq-attention, we get a new vector that could live in many of different, almost orthogonal subspaces. In the case of the Selective SSM, every \"attention head\" only writes a single coordinate. Of course, there is a dependence on the other coordinates via $\\Delta$, but due to the softplus function, it serves rather as a *gate* than as a linear transformation. Therefore, one can speculate that the SSM has access to as many orthogonal subspaces as there are hidden dimensions, whereas the usual attention can utilize exponentially more. This might seem as a plausible explanation of why we do not observe the phase transition in the SSMs. However, we need to be very cautious here. @eq-ssm-attention is very non-linear in $x$ so our intuition from the usual attention might be misleading.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
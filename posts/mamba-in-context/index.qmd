---
title: "Mamba and in-context learing: initial look"
# description: "Post description"
author: 
  - "Inar Timiryasov"
  - "Jean-Loup Tastet"
date: "2024-02-14"
bibliography: SSSM_refs.bib
draft: true
execute:
  echo: false
# reference-location: margin
# categories:
# - LLM
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
---


<!-- todo: 
- redo plots
- html with predictions
- cite the recent paper with vocab size = 40
-->

<!-- Global Matplotlib setup -->
```{python}
import matplotlib.pyplot as plt
theme_background_color = (252 / 255, 252 / 255, 252 / 255) # Match light color theme
plt.rcParams['figure.facecolor'] = theme_background_color
plt.rcParams['axes.facecolor'] = theme_background_color
#%config InlineBackend.figure_formats = ['svg'] # For vector graphics (sharper, lighter but possibly slower to render for busy plots)
plt.rcParams['figure.dpi'] = 150 # For raster graphics (less sharp, heavier, trivial to render)
```

# DRAFT 2024-02-14

## Introduction

Mamba [@gu2023mamba] is an architecture based on a Selective Structured State-Space model.
Recently it has taken the community by storm --- and for good reason. It features linear complexity in the sequence length and outperforms transformers of similar size in the language modelling task. It also benefits from a hardware-aware implementation. It has already been applied to tasks such as medical image segmentation [@ma2024u]. 
We are now applying Mamba to study ultra-high energy particles in a gigantic neutrino telescope in the South pole.^[This telescope is an instrumented cubic kilometer of ice, called IceCube. It has more than 5000 photodetectors that collect the data.
Transformers are in general well suited for this task. But their quadratic dependence on the input length doesn't allow to study the brightest events that have hundreds of thousands of pulses.]

Interestingly, the model design has been in part motivated by the mechanistic interpretability [@elhage2021mathematical] studies and specifically the idea of in-context learning and induction heads [@olsson2022context]. 

So here we wanted take a look at the in-context learning in Mamba. Meanwhile, there are already two papers with seemingly contradicting results. @akyurek2024context studied synthetic languages with small vocabulary size and found that transformers are superior to other models. On the other hand, @grazzi2024mamba find thath Mamba is capable of in-context learning. We wanted to take a look at this ourselves.



## In-Context Learning

By in-context learning we mean the ability of the model to learn during the inference time using the information from the context. This most clearly manifests itself as the decrease of the per-token loss as a function of the token position in the sequence, see @fig-in-context.

<!-- placeholder: create a better plot
![Loss per token as a function of token position, averaged over 200 sequences. Children stories.](fig/incontext-loss.png){fig-align="center" width=60% #fig-in-context} -->

```{python}
#| label: fig-in-context
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_stories_qwen.txt')

fig = plt.figure()
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```

Induction heads are believed to be central to in-context learning. So what are they?
Induction heads are circuits that allow the model to predict [B] after [A] if the pair [A][B] has already appeared in the context.^[Think of [A] being "Harry" and [B] being "Potter". If "Harry Potter" was present in the context already, the model will predict "Potter" after "Harry" with high probability.]
Induction heads are already present in two layer attention-only transformers (but not in single layer ones). 

<!-- add predictions html-->

Mamba and the predecessor model H3 [@fu2022hungry] have been designed with the idea of induction heads in mind.



## Mamba: phenomenological study
How can we check the in-context learning ability of Mamba?
One simple test is to feed the model random garbage, repeated twice. Quite amusingly, transformers are able to learn the pattern and predict the second half of the sequence with high accuracy, like in @fig-random-qwen. We used the smallest [Qwen-1.5](https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524) model mainly since it has long context window which will be used below. Even GPT-2 124M is perfectly capable of solving this task as far as the sequence fits into the context length.

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/qwen-random-tokens.png){fig-align="center" width=60% #fig-random-qwen} -->
```{python}
#| label: fig-random-qwen
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_800_qwen.txt')

fig = plt.figure()
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```


So what about Mamba? We used the [original](https://huggingface.co/state-spaces/mamba-790m) model which has been trained with 2k context length. Recently, [LongMamba](https://github.com/jzhang38/LongMamba) with 16k context has appeared, so we also used this [model](https://huggingface.co/PY007/LongMamba_16384_bs128_step400/tree/main). Here is the result:

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens.png){fig-align="center" width=60% #fig-random-mamba} -->
```{python}
#| label: fig-random-mamba
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_800_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_800_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_800_longmamba.txt')

fig = plt.figure()
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.plot(mamba_loss, label = 'Mamba-790m')
plt.plot(longmamba_loss, label = 'LongMamba-2.8B', alpha=0.7)
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```

Ok, what about even longer contexts?

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens-long.jpeg){fig-align="center" width=60% #fig-random-mamba-long} -->
```{python}
#| label: fig-random-mamba-long
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_8k_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_8k_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_8000_longmamba.txt')

fig = plt.figure()
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.plot(mamba_loss, label = 'Mamba-790m')
plt.plot(longmamba_loss, label = 'LongMamba-2.8B',alpha=0.7)
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```

Oops! So, we can see that a small transformer deals with this easily, but Mamba struggles. Anyway, this is quite relatable -- transformers are clearly superhuman in memorising random crap.

Notice that this is a very artificial test, though. The data is completely out of distribution.^[Garbage in --- garbage out. It is actually really impressive that transformers deal with this so easily.] One can speculate that perhaps mamba learns N-gram statistics too well and the induction heads cannot change the predictions. 

So what about normal texts? We used Children Stories dataset which was a part of the [BabyLM Challenge](https://babylm.github.io). All stories are in a single file, which we tokenized and split into sequences of 8000 tokens. So the beggining of the sequences is not sequences are not aligned with the text in a minigful way. This is probably the reason why the plots are noisy.
<!-- placeholder: create a better plot-->
<!-- ::: {#fig-losses layout-ncol=2}

![Qwen](fig/incontext-loss.png){width=90%}

![Mamba](fig/incontext-loss-mamba.png){width=90%}

Loss per token as a function of token position, averaged over 200 sequences. Children stories.
::: -->
```{python}
#| label: fig-losses-stories
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_stories_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_stories_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_stories_longmamba.txt')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)  
ax1.plot(qwen_loss, label='Qwen 1.5-0.5B')
ax1.set_ylim(1.5, 7)
ax1.set_xlabel('Token position')
ax1.set_ylabel('Per Token Loss')
ax1.legend()
ax1.set_title('Qwen Loss')



ax2.plot(longmamba_loss, label='LongMamba-2.8B', color='tab:green')
ax2.plot(mamba_loss, label='Mamba-790M', color='tab:orange', linestyle=':', alpha=0.7)
ax2.set_ylim(1.5, 7)
ax2.set_xlabel('Token position')
#ax2.set_ylabel('Per Token Loss')
ax2.legend()
ax2.set_title('Mamba Loss')


plt.tight_layout()
plt.show()
```

As one can see from @fig-losses-stories, the original Mamba strugles with longer sequnces. This is not so surprising since the model was trained with the 2k context length. LongMamba [post](https://github.com/jzhang38/LongMamba) has a nice intuition why this may happen. LonMamba performs on-par with transformer. So we do see the decrease of the per-token loss which signifies in-context learning. Can we quantify this?

## In-Context Learning During Training

TODO:

* discuss the observations by @olsson2022context

* ~~introduce the score~~

* Describe the models and the dataset.

* explain why separating ssm (see the next subsection)

### In-Context Learning Score

In order to follow the evolution of the in-context learning capabilities of the model as it trains, we follow @olsson2022context and introduce the so-called in-context learning score, define loosely as the difference between the mean per-token loss in late and early context. The precise definition that we use is:

```{python}
#| echo: true
def loss_decrease_score(losses):
    beg_sl = slice(30, 60)
    end_sl = slice(990, 1023)
    beginning_mean = losses[:, beg_sl].mean(axis=1)
    end_mean = losses[:, end_sl].mean(axis=1)
    scores = end_mean - beginning_mean
    return scores
```

The more negative this score is, the better the model is at leveraging the information present in its context when predicting the next tokens.

<!-- placeholder: create a better plot-->
<!-- ::: {#fig-scores layout-ncol=2}

![](fig/score.png){width=90%}

![](fig/loss.png){width=90%}

Score and eval loss.
::: -->

```{python}
#| label: fig-scores
#| fig-cap: In-context learning score and eval loss for Mamba and attention-only transformers.
import matplotlib.pyplot as plt
import numpy as np

seq_len = 1024
elapsed_tokens = seq_len*np.loadtxt('./data/training/elapsed_tokens.txt')

models = ['llama_1', 'llama_2', 'mamba_1', 'mamba_2']
labels = ['Llama 1 layer', 'Llama 2 layers', 'Mamba 1 layer', 'Mamba 2 layers']
styles = [
    {'color': 'tab:blue'  , 'linestyle': '-' },
    {'color': 'tab:blue'  , 'linestyle': '--'},
    {'color': 'tab:orange', 'linestyle': '-' },
    {'color': 'tab:orange', 'linestyle': '--'},
]
eval_losses = [np.loadtxt(f'./data/training/eval_losses_{model}.txt') for model in models]
per_token_losses = [np.loadtxt(f'./data/training/per_token_losses_{model}.txt') for model in models]

scores = [loss_decrease_score(loss) for loss in per_token_losses]

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=True) 

fig.subplots_adjust(hspace=0.15)

for model, score, sty in zip(labels, scores, styles):
    ax1.plot(elapsed_tokens, score, label=model, **sty)

for model, eval_loss, sty in zip(labels, eval_losses, styles):
    ax2.plot(elapsed_tokens, eval_loss, label=model, **sty)

#plt.tight_layout()  
#ax1.set_xlabel('Elapsed tokens')
ax1.set_ylabel('In-context Learning Score')
ax1.legend()
ax1.set_ylim(-0.25, 0.0)
#ax1.set_title('Score Decrease Over Training')

ax2.set_xlabel('Elapsed tokens')
ax2.set_ylabel('Eval Loss')
ax2.legend()
ax2.set_ylim(3.5, 5.)
#ax2.set_title('Eval Loss Over Training')


#plt.tight_layout()
plt.show()
```




## SSSM-only models


Mamba is more intricate than a transformer since it involves a selective state space model and gating, see [@gu2023mamba].
It is known, however, that attention-only transformers already exhibit interesting behavior [@elhage2021mathematical]. Can we, analogously, consider a model based on the SSM only, without convolutions and gating?

We have trained several models on PG-19 dataset [@raecompressive2019]. Those included: single-layer and two-layer attention-only transformers with rotary position encoding (RoPE), Mamba, and SSM-only models. The training of Mamba and attention-only transformers is shown in figure @fig-scores, while SSM-only models and attention-only transformers are compared in figure @fig-scores-ssm.


<!-- placeholder: create a better plot-->
<!-- ::: {#fig-scores-ssm layout-ncol=2}

![](fig/score-ssm.png){width=90%}

![](fig/loss-ssm.png){width=90%}

In-context learning score and eval loss for SSMs and attention-only transformers.
::: -->

```{python}
#| label: fig-scores-ssm
#| fig-cap: In-context learning score and eval loss for SSMs and attention-only transformers.
import matplotlib.pyplot as plt
import numpy as np

seq_len = 1024
elapsed_tokens = seq_len*np.loadtxt('./data/training/elapsed_tokens.txt')

models = ['llama_1', 'llama_2', 'ssm_1', 'ssm_2']
labels = ['Llama 1 layer', 'Llama 2 layers', 'SSM 1 layer', 'SSM 2 layers']
styles = [
    {'color': 'tab:blue', 'linestyle': '-' },
    {'color': 'tab:blue', 'linestyle': '--'},
    {'color': 'tab:red' , 'linestyle': '-' },
    {'color': 'tab:red' , 'linestyle': '--'},
]
eval_losses = [np.loadtxt(f'./data/training/eval_losses_{model}.txt') for model in models]
per_token_losses = [np.loadtxt(f'./data/training/per_token_losses_{model}.txt') for model in models]
 
scores = [loss_decrease_score(loss) for loss in per_token_losses]

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=True)

fig.subplots_adjust(hspace=0.15)

for model, score, sty in zip(labels, scores, styles):
    ax1.plot(elapsed_tokens, score, label=model, **sty)

for model, eval_loss, sty in zip(labels, eval_losses, styles):
    ax2.plot(elapsed_tokens, eval_loss, label=model, **sty)

#plt.tight_layout()  
#ax1.set_xlabel('Elapsed tokens')
ax1.set_ylabel('In-context Learning Score')
ax1.legend()
ax1.set_ylim(-0.25, 0.0)
#ax1.set_title('Score Decrease Over Training')

ax2.set_xlabel('Elapsed tokens')
ax2.set_ylabel('Eval Loss')
ax2.legend()
ax2.set_ylim(3.5, 5.)
#ax2.set_title('Eval Loss Over Training')


#plt.tight_layout()
plt.show()
```

Several observations are in order. First,  we can see that an SSM-only model is already pretty solid at language modelling. Secondly, both the single-layer attention-only transformer and the SSM struggle with in-context learning. This is expected since we know that induction heads can only form in two-layer models [@elhage2021mathematical]. At the same time, the SSM has a lower loss.^[We use the same tokenizer and the same model size, with only the learning rate tuned for each model separately.]
At two layers things become interesting. The transformer experiences a "phase transition" and the score quickly drops.^[Our plots agree very well with [@olsson2022context] except that the phase change happens earlier in training. We use a rather small vocab size of 16384 and perhaps much smaller batch size of 24 sequence of 2048 tokens.]
But for the 2-layer SSM the drop is far less significant. In fact, we have checked SSMs up to 16 layers and they cannot match the score of the two-layer attention-only transoformer. This is in contrast to the loss, which is lower for SSMs. One can speculate that selective SSMs are better at approximating N-gram statistics, but worse at in-context learning. Can we try to understand why is this the case? 


## Discussion

We have seen that Mamba is capable of in-context learning, but not as good as transformers. One **very handwavy** explanation is that Mamba has to compress all the information from the previous tokens into a single state. What is the size of this state? There is a hidden state vector (with default size `d_state=16`) per every coordinate of the input vector. The input vector has size `2 * d_model`^[This is because the embedding vector of every token is expanded by a factor of 2 before passing to the SSM, so the input vector is twice as large as the embedding vector.] Bare in mind also that those hidden state vectors do not talk to each other explicitly. So the total size of the state is `2 * d_state * d_model`. 

In the attention mechanism, on the other hand, we have to pass the KV cache from all previous tokens. This is `2 * d_model * context_length`. So the amount of information^[We are sloppy here. Transformers and SSM compress information in different ways, so the diensioanlity of the cache does not necessary transforms into information in the strict mathematical sense.] that attetnion passes to the next token surpasses that of SSM once the context is longer than `d_state`. Again, let us stress that this is a specualtion so far.
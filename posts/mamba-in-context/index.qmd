---
title: "Mamba and in-context learing: initial look"
# description: "Post description"
author: 
  - "Inar Timiryasov"
  - "Jean-Loup Tastet"
date: "2024-03-27"
bibliography: SSSM_refs.bib
draft: true
execute:
  echo: false
# reference-location: margin
# categories:
# - LLM
# format:
#   pdf:
#     toc: true
#     number-sections: true
#     colorlinks: true
---


<!-- todo: 
- redo plots
- html with predictions
- cite the recent paper with vocab size = 40
-->

<!-- Global Matplotlib setup -->
```{python}
import matplotlib.pyplot as plt
theme_background_color = (252 / 255, 252 / 255, 252 / 255) # Match light color theme
plt.rcParams['figure.facecolor'] = theme_background_color
plt.rcParams['axes.facecolor'] = theme_background_color
#%config InlineBackend.figure_formats = ['svg'] # For vector graphics (sharper, lighter but possibly slower to render for busy plots)
plt.rcParams['figure.dpi'] = 150 # For raster graphics (less sharp, heavier, trivial to render)
```

## Introduction

Mamba [@gu2023mamba] is a recently-introduced architecture based on a Selective Structured State-Space model (S6), that has taken the community by storm --- and for good reason. It features linear complexity in the sequence length, and was presented as a compelling alternative to the transformer architecture --- being one of the few models that can match or exceed the performance of similar-sized transformers on some language modelling tasks. It also greatly benefits from a hardware-aware implementation released along with the paper, thus enabling parallel training at speeds on par with transformers.
Mamba has already seen applications to tasks such as medical image segmentation [@ma2024u], and we are currently investigating its use to study ultra-high energy particles in the IceCube detector, a gigantic neutrino telescope located at the South pole.^[This telescope is an instrumented cubic kilometer of ice, containing more than 5000 photodetectors, that are designed to detect the Cherenkov light emitted by relativistic particles travelling through the ice.
Transformers are in general well suited for this task. But their quadratic dependence on the input length prevents us from studying the brightest events, that may contain hundreds of thousands of pulses.]

Interestingly, the model design has been motivated in part by the mechanistic interpretability studies [@elhage2021mathematical] and specifically by the idea of in-context learning and induction heads [@olsson2022context].

In the present post, we will take a look at in-context learning in Mamba. Meanwhile, there are already two papers out, with seemingly contradictory results. @akyurek2024context studied synthetic languages with small vocabulary size and found that transformers are superior to other models. On the other hand, @grazzi2024mamba find thath Mamba is capable of in-context learning. We wanted to take a look at this ourselves. **TODO: also discuss @aksenov2024linear?**



## In-Context Learning

By _in-context learning_ [@kaplan2020scaling;@brown2020language], we mean the ability of the model to learn at inference time, using the information from the context. This most clearly manifests itself as the decrease of the per-token loss as a function of the token position in the sequence, see @fig-in-context.

<!-- placeholder: create a better plot
![Loss per token as a function of token position, averaged over 200 sequences. Children stories.](fig/incontext-loss.png){fig-align="center" width=60% #fig-in-context} -->

```{python}
#| label: fig-in-context
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_stories_qwen.txt')

fig = plt.figure()
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.xlim(-15, 8000)
plt.legend()
plt.show(fig)
```

_Induction heads_ [@elhage2021mathematical;@olsson2022context] are believed to be central to in-context learning.
But what are they? Essentially, induction heads are circuits that allow the model to predict [B] after [A] if the pair [A][B] has already appeared in the context.
For instance, if the model has already encountered the string [A][B] = " Harry Potter", then, at the next occurence of [A] = " Harry", the induction head will predict an increased probability for [B] = " Potter".
Induction heads can already form in models as small as two-layer, attention-only transformers (but not in single-layer ones). 

<!-- add predictions html-->

Mamba and its predecessor model, H3 [@fu2022hungry], have been designed with the idea of induction heads in mind.
The present post aims to investigate whether this actually results in an improved in-context learning ability for Mamba, by comparing it to state-of-the-art models based on the transformer architecture. **TODO: also compare to S4?**


## Mamba: Phenomenological Study

### Repeating Random Tokens

How can we test the in-context learning ability of Mamba?
One simple test consists in feeding random tokens, repeated twice, to the model. Quite amusingly, transformers are able to learn the pattern and predict the second half of the sequence with high accuracy, as can be seen in @fig-random-qwen. We chose the smallest [Qwen-1.5](https://huggingface.co/collections/Qwen/qwen15-65c0a2f577b1ecb76d786524) model as our reference transformer, mainly for its long context window, which we will use below. Even GPT-2 124M is perfectly capable of solving this task, as long as the sequence can fit into its context window.

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/qwen-random-tokens.png){fig-align="center" width=60% #fig-random-qwen} -->
```{python}
#| label: fig-random-qwen
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_800_qwen.txt')

fig = plt.figure()
plt.autoscale(tight=True)
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```


Can Mamba do as well as transformers on this task? We tested both the [original model](https://huggingface.co/state-spaces/mamba-790m), which was trained with a 2k context length, as well as the newer [LongMamba](https://huggingface.co/PY007/LongMamba_16384_bs128_step400/tree/main), which was [further fine-tuned](https://github.com/jzhang38/LongMamba) with a 16k context. Here are the results:
**TODO: compare similar-sized models, in case it makes a difference?**

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens.png){fig-align="center" width=60% #fig-random-mamba} -->
```{python}
#| label: fig-random-mamba
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_800_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_800_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_800_longmamba.txt')

fig = plt.figure()
plt.autoscale(tight=True)
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.plot(mamba_loss, label = 'Mamba-790m')
plt.plot(longmamba_loss, label = 'LongMamba-2.8B', alpha=0.7)
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```

While the small transformer solves this task nearly perfectly, the two Mamba-based models can only reduce the loss by $\approx 8\,\mathrm{nats}$. The loss increases with the token position in the repeated sequence (less so with LongMamba), suggesting that the model has a limited capacity to memorize previous tokens. **TODO: check if this sounds correct** **JL: I am still not fully convinced by this interpretation: why can the model initially remember (at least approximately) tokens that are 4000 tokens back (in @fig-random-mamba-long), but fails to do so later in the context, while the distance between the original and repeated token always remains the same? (=4000)**

What about even longer sequences? Letâ€™s repeat this test with a sequence of 4000 random tokens, repeated twice.

<!-- placeholder: create a better plot-->
<!-- ![Loss per token as a function of token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.](fig/mamba-random-tokens-long.jpeg){fig-align="center" width=60% #fig-random-mamba-long} -->
```{python}
#| label: fig-random-mamba-long
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. First half of the tokens is randomly generated, the second half is just a repetition of the first one.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_8k_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_8k_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_8000_longmamba.txt')

fig = plt.figure()
plt.autoscale(tight=True)
plt.plot(qwen_loss, label = 'Qwen1.5-0.5B')
plt.plot(mamba_loss, label = 'Mamba-790m')
plt.plot(longmamba_loss, label = 'LongMamba-2.8B',alpha=0.7)
plt.xlabel('Token position')
plt.ylabel('Per Token Loss')
plt.legend()
plt.show(fig)
```

Oops! While the small transformer can still deal with this task easily, both Mambas struggle, with the loss of Mamba exploding after about 1500-2000 tokens (which, incidentally, is similar to the 2k sequence length it was trained on). **TODO: try LongMamba again, this time with a 2 x 32k sequence length, to see if it explodes** This makes the Mambas somehow more relatable --- after all, transformers' ability to memorize random sequences is clearly superhuman.

Let us emphasize that this is a very artificial test, with random sequences being completely out of distribution.^[Garbage in --- garbage out. It is actually really impressive that transformers deal with this task so easily.] One can speculate that perhaps Mamba learns N-gram statistics too well and the induction heads cannot change the predictions. **FIXME: this last sentence sounds a bit out of context. We should try to motivate it better**

### Natural Language

Since random tokens may be too far out of distribution, what about in-context learning with natural text? To compare the models in this more realistic setting, we used the Children Stories dataset, which was used as part of the [BabyLM Challenge](https://babylm.github.io). All the stories are contained within a single file, which we tokenized and split into sequences of 8000 tokens.^[Because of this naive splitting, the sequences are not aligned with the individual stories in a meaningful way, which may contribute to making the plots slightly noisier.]
<!-- placeholder: create a better plot-->
<!-- ::: {#fig-losses layout-ncol=2}

![Qwen](fig/incontext-loss.png){width=90%}

![Mamba](fig/incontext-loss-mamba.png){width=90%}

Loss per token as a function of token position, averaged over 200 sequences. Children stories.
::: -->
```{python}
#| label: fig-losses-stories
#| fig-cap: Loss per token as a function of the token position, averaged over 200 sequences. Children stories dataset.
import matplotlib.pyplot as plt
import numpy as np

qwen_loss = np.loadtxt('./data/losses_stories_qwen.txt')
mamba_loss = np.loadtxt('./data/losses_stories_mamba.txt')
longmamba_loss = np.loadtxt('./data/losses_stories_longmamba.txt')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey=True)  
ax1.plot(qwen_loss, label='Qwen 1.5-0.5B')
ax1.set_ylim(1.5, 7)
ax1.set_xlabel('Token position')
ax1.set_ylabel('Per Token Loss')
ax1.legend()
ax1.set_title('Qwen Loss')
ax1.set_xlim(-30, 8000)


ax2.plot(longmamba_loss, label='LongMamba-2.8B', color='tab:green')
ax2.plot(mamba_loss, label='Mamba-790M', color='tab:orange', linestyle=':', alpha=0.7)
ax2.set_ylim(1.5, 7)
ax2.set_xlabel('Token position')
#ax2.set_ylabel('Per Token Loss')
ax2.legend()
ax2.set_title('Mamba Loss')
ax2.set_xlim(-30, 8000)


plt.tight_layout()
plt.show()
```

As one can see from @fig-losses-stories, the original Mamba struggles with longer sequences. This is not so surprising, since the model was trained with only a 2k context length. The [LongMamba post](https://github.com/jzhang38/LongMamba) has a nice intuitive explanation of why this may happen. **TODO: give a tiny bit more details about what this explanation consists in** On the other hand, the version that was fine-tuned on longer sequences --- LongMamba --- performs on par with the transformer, and in both cases we see a decrease of the per-token loss, which shows that in-context learning is happening. Can we quantify it?

## In-Context Learning During Training

<!--
**TODO:**

* **discuss the observations by @olsson2022context**

* **~~introduce the score~~**

* **Describe the models ~~and the dataset~~.**

* **explain why separating ssm (see the next subsection -> I pooled it into the same section)**
-->

<!-- ### In-Context Learning Score -->

In order to follow the evolution of the in-context learning capabilities of the model as it trains, we follow @olsson2022context and introduce the so-called _in-context learning score_, define loosely as the difference between the mean per-token loss in late and early context. The precise definition that we use is:

```{python}
#| echo: true
def in_context_learning_score(losses: np.ndarray):
    early_ctx_sl = slice(30, 60)
    late_ctx_sl = slice(990, 1023)
    early_ctx_mean = losses[:, early_ctx_sl].mean(axis=1)
    late_ctx_mean = losses[:, late_ctx_sl].mean(axis=1)
    return late_ctx_mean - early_ctx_mean
```

The more negative this score is, the better the model is at leveraging the information present in its context when predicting the next tokens.

We trained several models on the PG-19 dataset [@raecompressive2019], which consists of a subset of books from the Project Gutenberg that were published before 1919, amounting to about 2 billion tokens in total.
The models include single-layer and two-layer attention-only transformers with rotary position encoding (RoPE), Mamba, and SSM-only models.

Compared to a transformer, Mamba is a more intricate model, since it involves not just a selective state-space model, but also gating, see @gu2023mamba.
It is known, however, that "bare bones" attention-only transformers already exhibit interesting behavior [@elhage2021mathematical].
In this spirit, we add to the comparison a model based on the selective SSM only (S6), but without the convolutions or gating.

The training of Mamba and attention-only transformers is shown in figure @fig-scores, while SSM-only models and attention-only transformers are compared in @fig-scores-ssm.

<!-- placeholder: create a better plot-->
<!-- ::: {#fig-scores layout-ncol=2}

![](fig/score.png){width=90%}

![](fig/loss.png){width=90%}

Score and eval loss.
::: -->

```{python}
#| label: fig-scores
#| fig-cap: In-context learning score and eval loss for Mamba and attention-only transformers.
import matplotlib.pyplot as plt
import numpy as np

seq_len = 1024
elapsed_tokens = seq_len*np.loadtxt('./data/training/elapsed_tokens.txt')

models = ['llama_1', 'llama_2', 'mamba_1', 'mamba_2']
labels = ['Llama 1 layer', 'Llama 2 layers', 'Mamba 1 layer', 'Mamba 2 layers']
styles = [
    {'color': 'tab:blue'  , 'linestyle': '-' },
    {'color': 'tab:blue'  , 'linestyle': '--'},
    {'color': 'tab:orange', 'linestyle': '-' },
    {'color': 'tab:orange', 'linestyle': '--'},
]
eval_losses = [np.loadtxt(f'./data/training/eval_losses_{model}.txt') for model in models]
per_token_losses = [np.loadtxt(f'./data/training/per_token_losses_{model}.txt') for model in models]

scores = [in_context_learning_score(loss) for loss in per_token_losses]

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=True) 

fig.subplots_adjust(hspace=0.15)

for model, score, sty in zip(labels, scores, styles):
    ax1.plot(elapsed_tokens, score, label=model, **sty)

for model, eval_loss, sty in zip(labels, eval_losses, styles):
    ax2.plot(elapsed_tokens, eval_loss, label=model, **sty)

#plt.tight_layout()  
#ax1.set_xlabel('Elapsed tokens')
ax1.set_ylabel('In-context Learning Score')
ax1.legend()
ax1.set_ylim(-0.25, 0.0)
#ax1.set_title('Score Decrease Over Training')

ax2.set_xlabel('Elapsed tokens')
ax2.set_ylabel('Eval Loss')
ax2.legend()
ax2.set_ylim(3.5, 5.)
#ax2.set_title('Eval Loss Over Training')

for ax in [ax1, ax2]:
    ax.autoscale(axis='x', tight=True)

#plt.tight_layout()
plt.show()
```


<!-- placeholder: create a better plot-->
<!-- ::: {#fig-scores-ssm layout-ncol=2}

![](fig/score-ssm.png){width=90%}

![](fig/loss-ssm.png){width=90%}

In-context learning score and eval loss for SSMs and attention-only transformers.
::: -->

```{python}
#| label: fig-scores-ssm
#| fig-cap: In-context learning score and eval loss for SSMs and attention-only transformers.
import matplotlib.pyplot as plt
import numpy as np

seq_len = 1024
elapsed_tokens = seq_len*np.loadtxt('./data/training/elapsed_tokens.txt')

models = ['llama_1', 'llama_2', 'ssm_1', 'ssm_2']
labels = ['Llama 1 layer', 'Llama 2 layers', 'SSM 1 layer', 'SSM 2 layers']
styles = [
    {'color': 'tab:blue', 'linestyle': '-' },
    {'color': 'tab:blue', 'linestyle': '--'},
    {'color': 'tab:red' , 'linestyle': '-' },
    {'color': 'tab:red' , 'linestyle': '--'},
]
eval_losses = [np.loadtxt(f'./data/training/eval_losses_{model}.txt') for model in models]
per_token_losses = [np.loadtxt(f'./data/training/per_token_losses_{model}.txt') for model in models]
 
scores = [in_context_learning_score(loss) for loss in per_token_losses]

fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=True)

fig.subplots_adjust(hspace=0.15)

for model, score, sty in zip(labels, scores, styles):
    ax1.plot(elapsed_tokens, score, label=model, **sty)

for model, eval_loss, sty in zip(labels, eval_losses, styles):
    ax2.plot(elapsed_tokens, eval_loss, label=model, **sty)

#plt.tight_layout()  
#ax1.set_xlabel('Elapsed tokens')
ax1.set_ylabel('In-context Learning Score')
ax1.legend()
ax1.set_ylim(-0.25, 0.0)
#ax1.set_title('Score Decrease Over Training')

ax2.set_xlabel('Elapsed tokens')
ax2.set_ylabel('Eval Loss')
ax2.legend()
ax2.set_ylim(3.5, 5.)
#ax2.set_title('Eval Loss Over Training')

for ax in [ax1, ax2]:
    ax.autoscale(axis='x', tight=True)

#plt.tight_layout()
plt.show()
```

Several observations are in order. First, we can see that an SSM-only model already shows solid language modelling performance, with a loss lower or equal to that of Llama.^[We use the same tokenizer and the same model size, with only the learning rate tuned for each model separately.] Second, both single-layer models --- either the attention-only transformer or the SSM --- struggle with in-context learning. This is expected since we know that induction heads can only form in two-layer models [@elhage2021mathematical]. **TODO: clarify why 1-layer Mamba is a bit better here (conv1d? try ablating it?)**

At two layers, things become interesting. The transformer experiences a "phase transition" (previously observed by @olsson2022context) where the score quickly drops as induction heads form.^[Our plots agree very well with [@olsson2022context] except that the phase change happens earlier in training. We use a rather small vocab size of 16384 and perhaps a much smaller batch size, of 24 sequence of 2048 tokens each.]
But for the two-layer SSM, the drop is far less significant. In fact, we have tested models with up to 16 S6 layers, and they cannot match the score of the two-layer attention-only transformer. This is in contrast to the loss, which remains lower for SSMs. This leads us to speculate that selective SSMs are better at approximating N-gram statistics, but worse at in-context learning. Can we try to understand why this is the case? 


## Discussion

In this post, we have seen that Mamba is capable of in-context learning, but is not as good at this task as transformers. One potential (and _very_ handwavy) explanation is that Mamba has to compress all the information from the previous tokens into a single state. What is the size of this state? Each coordinate from the _input vector_ is associated to a hidden-state vector (with default size `d_state=16`). The input vector itself has size `2 * d_model`.^[This is because the embedding vector of every token is expanded by a factor of 2 before passing it to the SSM, so the input vector is twice as large as the embedding vector.] This leads to a total size of `2 * d_state * d_model` for the state. Keep in mind that those hidden-state vectors do not talk to each other explicitly. **TODO: is the last sentence needed? If yes, clarify it**

On the other hand, in the self-attention mechanism, we have to pass the KV cache from all previous tokens in order to predict the next one. This would correspond to a "state" of size `2 * d_model * context_length`. So the amount of information^[We are being sloppy here. Transformers and SSMs compress information in different ways, so the dimensionality of the cache does not necessarily map to information in the strict mathematical sense.] that self-attention passes to the next token surpasses that of the SSM once the context is longer than `d_state`. Again, let us stress that this so far remains speculative.